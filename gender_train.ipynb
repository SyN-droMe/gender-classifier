{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6NvwMm-dPDk",
        "outputId": "087c8db1-6905-4c4e-aee3-f7a6431883cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqEqYCaQdFzx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train_data_dir = '/content/drive/MyDrive/gender_dataset_full/gender_full_body'\n",
        "#val_data_dir = '/content/drive/MyDrive/gender_dataset_full/gender_full_body'\n",
        "train_data_dir = '/content/drive/MyDrive/Dataset'\n",
        "val_data_dir = '/content/drive/MyDrive/Dataset'"
      ],
      "metadata": {
        "id": "vsoaq99cdwtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ],
      "metadata": {
        "id": "ihqqjRM9eSYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_datasets = {x: datasets.ImageFolder(os.path.join(train_data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
        "dataloaders = {x: DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI37_FCOeS6t",
        "outputId": "76d4f454-f9ea-40b3-ef4c-5f8718e76ae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet18(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(256, 6),  # Two output nodes for male and female 2->6\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n"
      ],
      "metadata": {
        "id": "MAw8Rj6aeZS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 50  #7->50\n",
        "save_model_path = '/content/drive/MyDrive/components_new.pth'\n",
        "onnx_model_path = '/content/drive/MyDrive/components_new.onnx'\n",
        "#save_model_path = '/content/drive/MyDrive/second_trial_new.pth'\n",
        "#onnx_model_path = '/content/drive/MyDrive/second_trial_new.onnx'  # Specify the path for saving the ONNX model\n",
        "best_val_loss = float('inf')\n",
        "early_stopping_counter = 0\n",
        "patience = 3  # You can adjust this value based on your preference for early stopping\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    for phase in ['train', 'val']:\n",
        "        if phase == 'train':\n",
        "            model.train()\n",
        "        else:\n",
        "            model.eval()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in dataloaders[phase]:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        if phase == 'train':\n",
        "            exp_lr_scheduler.step()\n",
        "\n",
        "        epoch_loss = running_loss / dataset_sizes[phase]\n",
        "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "        print(f'{phase} Epoch {epoch}/{num_epochs} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "        if phase == 'val':\n",
        "            if epoch % 5 == 0:  # Save the model every 5th epoch\n",
        "                torch.save(model.state_dict(), f'/content/drive/MyDrive/Dataset/checkpoints/checkpoint2_{epoch}.pth')\n",
        "\n",
        "            # Early stopping check\n",
        "            if epoch > 1 and epoch_loss < best_val_loss:\n",
        "                best_val_loss = epoch_loss\n",
        "                early_stopping_counter = 0\n",
        "            else:\n",
        "                early_stopping_counter += 1\n",
        "\n",
        "            if early_stopping_counter >= patience:\n",
        "                print(\"Early stopping triggered\")\n",
        "                break\n",
        "\n",
        "# Save the final model\n",
        "torch.save(model.state_dict(), save_model_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdPt2sgvfbFG",
        "outputId": "905af335-92fc-463f-9f3e-e0cbfd4e5038"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 1/50 Loss: 1.2104 Acc: 0.5920\n",
            "val Epoch 1/50 Loss: 2.0064 Acc: 0.4256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 2/50 Loss: 1.1016 Acc: 0.6316\n",
            "val Epoch 2/50 Loss: 1.7535 Acc: 0.4414\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 3/50 Loss: 1.0827 Acc: 0.6397\n",
            "val Epoch 3/50 Loss: 1.3370 Acc: 0.4700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 4/50 Loss: 1.0239 Acc: 0.6499\n",
            "val Epoch 4/50 Loss: 1.4781 Acc: 0.4591\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 5/50 Loss: 1.0072 Acc: 0.6517\n",
            "val Epoch 5/50 Loss: 1.3673 Acc: 0.4581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 6/50 Loss: 1.0179 Acc: 0.6478\n",
            "val Epoch 6/50 Loss: 1.4282 Acc: 0.4916\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 7/50 Loss: 0.9545 Acc: 0.6662\n",
            "val Epoch 7/50 Loss: 1.3627 Acc: 0.5054\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 8/50 Loss: 0.8675 Acc: 0.7001\n",
            "val Epoch 8/50 Loss: 1.1707 Acc: 0.5419\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 9/50 Loss: 0.8167 Acc: 0.7164\n",
            "val Epoch 9/50 Loss: 1.1665 Acc: 0.5468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 10/50 Loss: 0.7934 Acc: 0.7312\n",
            "val Epoch 10/50 Loss: 1.1467 Acc: 0.5557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 11/50 Loss: 0.8027 Acc: 0.7227\n",
            "val Epoch 11/50 Loss: 1.1059 Acc: 0.5685\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 12/50 Loss: 0.7841 Acc: 0.7291\n",
            "val Epoch 12/50 Loss: 1.0951 Acc: 0.5567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 13/50 Loss: 0.7377 Acc: 0.7418\n",
            "val Epoch 13/50 Loss: 1.0793 Acc: 0.5773\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 14/50 Loss: 0.7351 Acc: 0.7400\n",
            "val Epoch 14/50 Loss: 1.0789 Acc: 0.5882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 15/50 Loss: 0.7258 Acc: 0.7457\n",
            "val Epoch 15/50 Loss: 1.0856 Acc: 0.5842\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 16/50 Loss: 0.7232 Acc: 0.7474\n",
            "val Epoch 16/50 Loss: 1.0909 Acc: 0.5823\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 17/50 Loss: 0.7080 Acc: 0.7425\n",
            "val Epoch 17/50 Loss: 1.0765 Acc: 0.5901\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 18/50 Loss: 0.7105 Acc: 0.7481\n",
            "val Epoch 18/50 Loss: 1.0801 Acc: 0.5852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 19/50 Loss: 0.7129 Acc: 0.7471\n",
            "val Epoch 19/50 Loss: 1.0797 Acc: 0.5852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 20/50 Loss: 0.7072 Acc: 0.7573\n",
            "val Epoch 20/50 Loss: 1.0628 Acc: 0.5892\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 21/50 Loss: 0.7047 Acc: 0.7549\n",
            "val Epoch 21/50 Loss: 1.0545 Acc: 0.5892\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 22/50 Loss: 0.7060 Acc: 0.7538\n",
            "val Epoch 22/50 Loss: 1.0569 Acc: 0.5941\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 23/50 Loss: 0.6906 Acc: 0.7517\n",
            "val Epoch 23/50 Loss: 1.0411 Acc: 0.5921\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 24/50 Loss: 0.6937 Acc: 0.7496\n",
            "val Epoch 24/50 Loss: 1.0721 Acc: 0.5862\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 25/50 Loss: 0.7015 Acc: 0.7527\n",
            "val Epoch 25/50 Loss: 1.0644 Acc: 0.5852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 26/50 Loss: 0.7219 Acc: 0.7467\n",
            "val Epoch 26/50 Loss: 1.0689 Acc: 0.5921\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 27/50 Loss: 0.7051 Acc: 0.7527\n",
            "val Epoch 27/50 Loss: 1.0767 Acc: 0.5911\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 28/50 Loss: 0.6952 Acc: 0.7563\n",
            "val Epoch 28/50 Loss: 1.0558 Acc: 0.5882\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 29/50 Loss: 0.6989 Acc: 0.7552\n",
            "val Epoch 29/50 Loss: 1.0676 Acc: 0.5901\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 30/50 Loss: 0.6989 Acc: 0.7573\n",
            "val Epoch 30/50 Loss: 1.0698 Acc: 0.5911\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 31/50 Loss: 0.7063 Acc: 0.7510\n",
            "val Epoch 31/50 Loss: 1.0580 Acc: 0.5892\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 32/50 Loss: 0.6971 Acc: 0.7605\n",
            "val Epoch 32/50 Loss: 1.0604 Acc: 0.5911\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 33/50 Loss: 0.6824 Acc: 0.7630\n",
            "val Epoch 33/50 Loss: 1.0572 Acc: 0.5882\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 34/50 Loss: 0.6839 Acc: 0.7665\n",
            "val Epoch 34/50 Loss: 1.0655 Acc: 0.5941\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 35/50 Loss: 0.6823 Acc: 0.7598\n",
            "val Epoch 35/50 Loss: 1.0486 Acc: 0.5901\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 36/50 Loss: 0.6982 Acc: 0.7534\n",
            "val Epoch 36/50 Loss: 1.0393 Acc: 0.5921\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 37/50 Loss: 0.6988 Acc: 0.7531\n",
            "val Epoch 37/50 Loss: 1.0758 Acc: 0.5862\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 38/50 Loss: 0.6900 Acc: 0.7503\n",
            "val Epoch 38/50 Loss: 1.0700 Acc: 0.5941\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 39/50 Loss: 0.6947 Acc: 0.7609\n",
            "val Epoch 39/50 Loss: 1.0683 Acc: 0.5911\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 40/50 Loss: 0.7007 Acc: 0.7542\n",
            "val Epoch 40/50 Loss: 1.0545 Acc: 0.5921\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 41/50 Loss: 0.6959 Acc: 0.7492\n",
            "val Epoch 41/50 Loss: 1.0576 Acc: 0.5911\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 42/50 Loss: 0.7078 Acc: 0.7443\n",
            "val Epoch 42/50 Loss: 1.0431 Acc: 0.5921\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 43/50 Loss: 0.7049 Acc: 0.7506\n",
            "val Epoch 43/50 Loss: 1.0589 Acc: 0.5941\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 44/50 Loss: 0.7181 Acc: 0.7446\n",
            "val Epoch 44/50 Loss: 1.0709 Acc: 0.5862\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 45/50 Loss: 0.6874 Acc: 0.7573\n",
            "val Epoch 45/50 Loss: 1.0509 Acc: 0.5941\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 46/50 Loss: 0.7100 Acc: 0.7527\n",
            "val Epoch 46/50 Loss: 1.0577 Acc: 0.5911\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 47/50 Loss: 0.6771 Acc: 0.7556\n",
            "val Epoch 47/50 Loss: 1.0617 Acc: 0.5911\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 48/50 Loss: 0.6948 Acc: 0.7640\n",
            "val Epoch 48/50 Loss: 1.0634 Acc: 0.5941\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 49/50 Loss: 0.7060 Acc: 0.7542\n",
            "val Epoch 49/50 Loss: 1.0694 Acc: 0.5921\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 50/50 Loss: 0.7124 Acc: 0.7439\n",
            "val Epoch 50/50 Loss: 1.0708 Acc: 0.5872\n",
            "Early stopping triggered\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4X_w0ayYiX-W",
        "outputId": "4ae9ee5b-afa2-4692-9e36-95da688fbbb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Export the model to ONNX format\n",
        "dummy_input = torch.randn(1, 3, 224, 224).to(device)  # Input size should match your network's input size\n",
        "torch.onnx.export(model, dummy_input, onnx_model_path, verbose=True)\n"
      ],
      "metadata": {
        "id": "ahECZJvziQ-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "checkpoint_path = '/content/drive/MyDrive/gender_dataset_full/checkpoints/checkpoint2_5.pth'\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(torch.load(f'/content/drive/MyDrive/gender_dataset_full/checkpoints/checkpoint2_5.pth'))\n",
        "    #optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    #exp_lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    #start_epoch = checkpoint['epoch'] + 1  # Start from the next epoch\n",
        "    #best_val_loss = checkpoint['best_val_loss']\n",
        "    start_epoch = 5\n",
        "else:\n",
        "    start_epoch = 0\n",
        "    best_val_loss = float('inf')"
      ],
      "metadata": {
        "id": "cxRI6VRP77S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 15  # Increased the number of epochs for demonstration\n",
        "save_model_path = '/content/drive/MyDrive/second_trial_new.pth'\n",
        "onnx_model_path = '/content/drive/MyDrive/second_trial_new.onnx'  # Specify the path for saving the ONNX model\n",
        "best_val_loss = float('inf')\n",
        "early_stopping_counter = 0\n",
        "patience = 3  # You can adjust this value based on your preference for early stopping\n",
        "\n",
        "for epoch in range(start_epoch, start_epoch + num_epochs):\n",
        "    for phase in ['train', 'val']:\n",
        "        if phase == 'train':\n",
        "            model.train()\n",
        "        else:\n",
        "            model.eval()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in dataloaders[phase]:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        if phase == 'train':\n",
        "            exp_lr_scheduler.step()\n",
        "\n",
        "        epoch_loss = running_loss / dataset_sizes[phase]\n",
        "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "        print(f'{phase} Epoch {epoch}/{start_epoch + num_epochs - 1} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "        if phase == 'val':\n",
        "            # Save the model every 5th epoch\n",
        "            #if epoch % 5 == 0:\n",
        "            #    torch.save({\n",
        "            #        'epoch': epoch,\n",
        "            #        'model_state_dict': model.state_dict(),\n",
        "            #        'optimizer_state_dict': optimizer.state_dict(),\n",
        "            #        'scheduler_state_dict': exp_lr_scheduler.state_dict(),\n",
        "            #        'best_val_loss': best_val_loss\n",
        "            #    }, f'gender_classification_model_epoch{epoch}.pth')\n",
        "\n",
        "            # Early stopping check\n",
        "            if epoch_loss < best_val_loss:\n",
        "                best_val_loss = epoch_loss\n",
        "            else:\n",
        "                patience -= 1\n",
        "\n",
        "            if patience == 0:\n",
        "                print(\"Early stopping triggered\")\n",
        "                break\n",
        "            if (epoch + 1) % 1 == 0:\n",
        "                torch.save(model.state_dict(), f'/content/drive/MyDrive/gender_dataset_full/checkpoints/checkpoint2_{epoch+1}.pth')\n",
        "\n",
        "\n",
        "# Save the final model\n",
        "torch.save(model.state_dict(), save_model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 983
        },
        "id": "OvVoGyqB7w_Y",
        "outputId": "29c11bb0-31ca-41a9-8361-bc6bcd598d53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Epoch 5/19 Loss: 0.3352 Acc: 0.8471\n",
            "val Epoch 5/19 Loss: 0.2157 Acc: 0.9082\n",
            "train Epoch 6/19 Loss: 0.3085 Acc: 0.8554\n",
            "val Epoch 6/19 Loss: 0.2467 Acc: 0.8869\n",
            "train Epoch 7/19 Loss: 0.3075 Acc: 0.8601\n",
            "val Epoch 7/19 Loss: 0.1996 Acc: 0.9104\n",
            "train Epoch 8/19 Loss: 0.2829 Acc: 0.8717\n",
            "val Epoch 8/19 Loss: 0.1899 Acc: 0.9180\n",
            "train Epoch 9/19 Loss: 0.2784 Acc: 0.8722\n",
            "val Epoch 9/19 Loss: 0.2045 Acc: 0.9130\n",
            "train Epoch 10/19 Loss: 0.2748 Acc: 0.8802\n",
            "val Epoch 10/19 Loss: 0.1779 Acc: 0.9203\n",
            "train Epoch 11/19 Loss: 0.2605 Acc: 0.8811\n",
            "val Epoch 11/19 Loss: 0.1739 Acc: 0.9281\n",
            "train Epoch 12/19 Loss: 0.2204 Acc: 0.9050\n",
            "val Epoch 12/19 Loss: 0.1442 Acc: 0.9377\n",
            "train Epoch 13/19 Loss: 0.1973 Acc: 0.9165\n",
            "val Epoch 13/19 Loss: 0.1385 Acc: 0.9402\n",
            "train Epoch 14/19 Loss: 0.1887 Acc: 0.9172\n",
            "val Epoch 14/19 Loss: 0.1348 Acc: 0.9467\n",
            "train Epoch 15/19 Loss: 0.1908 Acc: 0.9197\n",
            "val Epoch 15/19 Loss: 0.1367 Acc: 0.9427\n",
            "Early stopping triggered\n",
            "train Epoch 16/19 Loss: 0.1853 Acc: 0.9181\n",
            "val Epoch 16/19 Loss: 0.1427 Acc: 0.9405\n",
            "train Epoch 17/19 Loss: 0.1833 Acc: 0.9210\n",
            "val Epoch 17/19 Loss: 0.1336 Acc: 0.9453\n",
            "train Epoch 18/19 Loss: 0.1724 Acc: 0.9266\n",
            "val Epoch 18/19 Loss: 0.1353 Acc: 0.9461\n",
            "train Epoch 19/19 Loss: 0.1700 Acc: 0.9255\n",
            "val Epoch 19/19 Loss: 0.1317 Acc: 0.9467\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OnnxExporterError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/onnx_proto_utils.py\u001b[0m in \u001b[0;36m_add_onnxscript_fn\u001b[0;34m(model_bytes, custom_opsets)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'onnx'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOnnxExporterError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-8507f85ab3b5>\u001b[0m in \u001b[0;36m<cell line: 75>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# Export the model to ONNX format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mdummy_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Input size should match your network's input size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monnx_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \"\"\"\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m     _export(\n\u001b[0m\u001b[1;32m    517\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1668\u001b[0m                 )\n\u001b[1;32m   1669\u001b[0m             \u001b[0;31m# insert function_proto into model_proto.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1670\u001b[0;31m             proto = onnx_proto_utils._add_onnxscript_fn(\n\u001b[0m\u001b[1;32m   1671\u001b[0m                 \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1672\u001b[0m                 \u001b[0mcustom_opsets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/onnx_proto_utils.py\u001b[0m in \u001b[0;36m_add_onnxscript_fn\u001b[0;34m(model_bytes, custom_opsets)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOnnxExporterError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Module onnx is not installed!\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;31m# For > 2GB model, onnx.load_fromstring would fail. However, because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOnnxExporterError\u001b[0m: Module onnx is not installed!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx\n",
        "import onnx\n",
        "\n",
        "# Export the model to ONNX format\n",
        "dummy_input = torch.randn(1, 3, 224, 224).to(device)  # Input size should match your network's input size\n",
        "torch.onnx.export(model, dummy_input, onnx_model_path, verbose=True)"
      ],
      "metadata": {
        "id": "UnKR8c0XIH9P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}